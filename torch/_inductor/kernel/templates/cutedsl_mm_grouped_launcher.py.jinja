import functools
from torch._inductor.runtime.runtime_utils import ceildiv
from cutlass.utils import TensorMapUpdateMode
import cutlass.cute.runtime as cute_rt #TODO(nikhilap) make this and below generated
import cutlass.torch as cutlass_torch
{{gen_defines()}}
# ---- Import GroupedGemm implementation, copied on PyTorch build from Cutlass repository: cutlass/examples/python/CuTeDSL/blackwell/grouped_gemm.py ----
from torch._inductor.kernel.templates.cutedsl_mm_grouped_kernel import (
    GroupedGemmKernel,
    _prepare_metatadata_host,
)

# Note about caching:
# Each instantiated CuTeDSL grouped GEMM kernel file generated by Inductor
# maintains its own local caching system. At this stage, all compile-time
# constexprs (e.g., TILE_M, TILE_N, CLUSTER_M/N, USE_2_CTA) and the kernel
# name itself ({{kernel_name}}) are permanently baked into the file, so they
# do not need to be included in any cache key.
#
# The caching mechanism is split into two levels:
#
#   1. prep_cache
#      Caches the compiled executor for build_group_ptrs_from_bases(). This
#      kernel depends only on the tensor shapes, strides, and dtypes of A/B/C,
#      and can therefore be safely reused across runs with different group
#      partitioning (`offs`).
#
#   2. gemm_cache
#      Caches the compiled Grouped GEMM executor. Its key extends the prep
#      cache key with hardware- and grid-specific parameters:
#      (prep_cache_key, max_active_clusters, total_num_clusters).
#      This is necessary because different `offs` tensors can change the
#      per-group problem sizes and thus alter `total_num_clusters`, which in
#      turn changes the grid shape and persistent scheduler configuration.
#      Kernels compiled for one grid cannot be safely reused for another.
#
#
# Additionally, note the @lru_cache decorator on get_hardware_info(). Empirically,
# hw.get_max_active_clusters() triggers significant MLIR recompilation overhead,
# despite depending only on the GPU type. We cache this function to mitigate
# redundant recompiles even when shape/stride/dtype cache misses force kernel
# regeneration. A follow-up study will investigate the root cause.

_FPROP = 0
_DGRAD = 1
_WGRAD = 2

_HARDWARE_INFO = None
_SCHEDULE_METATA_CACHE = {}
_PREP_KERNEL_CACHE = {}
_MAIN_KERNEL_CACHE = {}


@cute.kernel
def build_group_ptrs_from_bases_kernel(
    base_A_u64: cutlass.Int64,  # device addr of input_a (bytes)
    base_B_u64: cutlass.Int64,  # device addr of input_b (bytes)
    base_C_u64: cutlass.Int64,  # device addr of Output (bytes)
    offs: cute.Tensor,  # [G], cutlass.Int32/64 cumulative
    K: cutlass.Constexpr,
    N: cutlass.Constexpr,
    sizeof_element: cutlass.Int32,  # bytes
    # -------- STRIDES (in ELEMENTS) --------
    stride_A_m_elems: cutlass.Constexpr,  # A.stride(0)
    stride_A_k_elems: cutlass.Constexpr,  # A.stride(1)
    stride_B0_elems: cutlass.Constexpr,  # B.stride(0)
    stride_Bk_elems: cutlass.Constexpr,  # B.stride(1)
    stride_Bn_elems: cutlass.Constexpr,  # B.stride(2)
    stride_C_m_elems: cutlass.Constexpr,  # C.stride(0)
    stride_C_n_elems: cutlass.Constexpr,  # C.stride(1)
    # -------- CLUSTER CONFIG --------
    BLOCK_M: cutlass.Constexpr,
    BLOCK_N: cutlass.Constexpr,
    # -------- OUTPUTS --------
    out_ptrs: cute.Tensor,  # [G,3] cutlass.Int64: (A_ptr, B_ptr, C_ptr)
    out_problem: cute.Tensor,  # [G,4] cutlass.Int32: (m_g, n, k, 1)
    out_strides_abc: cute.Tensor,  # [G,3,2] cutlass.Int32 [[A_m,A_k],[B_n,B_k],[C_m,C_n]]
    output_total_num_clusters: cute.Tensor,
):
    tidx, _, _ = cute.arch.thread_idx()
    g = tidx

    m_beg_i32 = 0
    if g > 0:
        m_beg_i32 = offs[g - 1]
    m_end_i32 = offs[g]
    m_g_i32 = m_end_i32 - m_beg_i32

    a_byte_off = (
        cutlass.Int64(m_beg_i32) * stride_A_m_elems * cutlass.Int64(sizeof_element)
    )
    c_byte_off = (
        cutlass.Int64(m_beg_i32) * stride_C_m_elems * cutlass.Int64(sizeof_element)
    )
    b_byte_off = cutlass.Int64(g) * stride_B0_elems * cutlass.Int64(sizeof_element)

    # ---- pointers ----
    out_ptrs[g, 0] = base_A_u64 + a_byte_off
    out_ptrs[g, 1] = base_B_u64 + b_byte_off
    out_ptrs[g, 2] = base_C_u64 + c_byte_off

    # ---- (m, n, k, 1) ----
    out_problem[g, 0] = m_g_i32
    out_problem[g, 1] = N
    out_problem[g, 2] = K
    out_problem[g, 3] = cutlass.Int32(1)

    # ---- strides ----
    out_strides_abc[g, 0, 0] = cutlass.Int32(stride_A_m_elems)
    out_strides_abc[g, 0, 1] = cutlass.Int32(stride_A_k_elems)
    out_strides_abc[g, 1, 0] = cutlass.Int32(stride_Bn_elems)
    out_strides_abc[g, 1, 1] = cutlass.Int32(stride_Bk_elems)
    out_strides_abc[g, 2, 0] = cutlass.Int32(stride_C_m_elems)
    out_strides_abc[g, 2, 1] = cutlass.Int32(stride_C_n_elems)

    if tidx == 0:
        total_clusters = 0
        num_groups_static = out_problem.shape[0]

        for i in range(num_groups_static):
            # Re-calculate M size for group i
            i_beg = 0
            if i > 0: i_beg = offs[i - 1]
            i_end = offs[i]
            m_size = i_end - i_beg

            # Ceil Div for clusters
            m_clusters = (m_size + BLOCK_M - 1) // BLOCK_M
            n_clusters = (N + BLOCK_N - 1) // BLOCK_N

            total_clusters += m_clusters * n_clusters

        output_total_num_clusters[0] = cutlass.Int32(total_clusters)


@cute.jit
def launch_build_group_ptrs_from_bases(
    base_A_u64: cutlass.Int64,
    base_B_u64: cutlass.Int64,
    base_C_u64: cutlass.Int64,
    offs: cute.Tensor,
    G: cutlass.Constexpr,
    K: cutlass.Constexpr,
    N: cutlass.Constexpr,
    sizeof_element: cutlass.Constexpr,
    stride_A_m_elems: cutlass.Constexpr,
    stride_A_k_elems: cutlass.Constexpr,
    stride_B0_elems: cutlass.Constexpr,
    stride_Bk_elems: cutlass.Constexpr,
    stride_Bn_elems: cutlass.Constexpr,
    stride_C_m_elems: cutlass.Constexpr,
    stride_C_n_elems: cutlass.Constexpr,
    # -------- CLUSTER CONFIG --------
    BLOCK_M: cutlass.Constexpr,
    BLOCK_N: cutlass.Constexpr,
    out_ptrs: cute.Tensor,  # [G,3] cutlass.Int64
    out_problem: cute.Tensor,  # [G,4] cutlass.Int32
    out_strides_abc: cute.Tensor,  # [3,2] cutlass.Int32
    stream: cuda.CUstream,
    output_total_num_clusters: cute.Tensor,
):
    build_group_ptrs_from_bases_kernel(
        base_A_u64,
        base_B_u64,
        base_C_u64,
        offs,
        K,
        N,
        sizeof_element,
        stride_A_m_elems,
        stride_A_k_elems,
        stride_B0_elems,
        stride_Bk_elems,
        stride_Bn_elems,
        stride_C_m_elems,
        stride_C_n_elems,
        BLOCK_M,
        BLOCK_N,
        out_ptrs,
        out_problem,
        out_strides_abc,
        output_total_num_clusters,
    ).launch(grid=(1, 1, 1), block=(G, 1, 1), stream=stream)

{{def_kernel("input_a", "input_b", "input_a_offs")}}
    stream = cuda.CUstream(stream)

    #TODO(nikhilap) write comment explaining this
    problem_type = _FPROP

    input_b = input_b.transpose(1, 2)

    #after transpose:
    # sumM, K = input_a.shape
    # G, N, Kb = input_b.shape

    G, N, K = input_b.shape
    M = input_a.shape[0]


    print(input_b.stride())

    # Store problem sizes
    problem_sizes_gmnk = (G, M, N, K)
    num_groups = G

    # Pack tensors (Use original input_b, do not transpose)
    torch_tensors_abc = (input_a, input_b, {{get_output()}})

    # --- 3. Kernel Configurations ---
    mma_tiler_mn = (TILE_M, TILE_N)
    cluster_shape_mn = (CLUSTER_M, CLUSTER_N)
    use_2cta_instrs = USE_2_CTA
    acc_dtype = cutlass.Float32
    tensormap_update_mode = TENSORMAP_UPDATE_MODE
    output_accum = False #TODO(nikhilap) write comment explaining this

    # --- 4. Dynamic Layout Detection ---
    # Detect which dimension of B is contiguous (stride 1)
    # input_b is (G, N, K).
    # If stride(N) == 1 -> Leading dim is 1 (Col Major)
    # If stride(K) == 1 -> Leading dim is 2 (Row Major)

    if problem_type == _FPROP:
        shape_key = (G, None, N, K)
        leading_dims = (1, 1, 1)
    elif problem_type == _DGRAD:
        shape_key = (G, None, N, K)
        leading_dims = (1, 0, 1)
    elif problem_type == _WGRAD:
        shape_key = (G, M, N, None)
        leading_dims = (0, 0, 1)
    else:
        raise ValueError("Unrecognized problem type: ", problem_type)

    {# cute_tensors_abc = [
        from_dlpack(
            x.unsqueeze(-1).detach(),
            assumed_align=16,
        )#.mark_layout_dynamic(leading_dim=d)
        for x, d in zip(torch_tensors_abc, leading_dims)
    ] #}

    cute_tensors_abc = [
        from_dlpack(input_a.unsqueeze(-1), assumed_align=16),
        from_dlpack(input_b[0].unsqueeze(-1), assumed_align=16),
        from_dlpack({{get_output()}}.unsqueeze(-1), assumed_align=16),
    ]

    block_sizes = mma_tiler_mn
    strides_abc = tuple((x.stride(0), x.stride(1)) for x in torch_tensors_abc)
    ptrs_abc = tuple(
        cute_rt.make_ptr(
            x.element_type,
            x.data_ptr,
            cute.AddressSpace.gmem,
            assumed_align=16,
        )
        for x in cute_tensors_abc
    )

    device = torch_tensors_abc[0].device

    ptrs_t = torch.empty((G, 3), device=device, dtype=torch.int64)
    probs_t = torch.empty((G, 4), device=device, dtype=torch.int32)
    strides_t = torch.empty((G, 3, 2), device=device, dtype=torch.int32)
    ptrs = from_dlpack(ptrs_t)
    probs = from_dlpack(probs_t)
    strides = from_dlpack(strides_t)

    torch_num_total_clusters = torch.empty((1,), dtype=torch.int32, device=device)
    cute_num_total_clusters = from_dlpack(torch_num_total_clusters, assumed_align=16)

    cache_key = (
        problem_type,
        tuple(t.dtype for t in torch_tensors_abc),
        input_a_offs.dtype,
        shape_key,
        leading_dims,
        output_accum,
    )

    base_A_u64 = int(input_a.data_ptr())
    base_B_u64 = int(input_b.data_ptr())
    base_C_u64 = int({{get_output()}}.data_ptr())

    # TODO(shikaili): Move prepare metdata into main JIT function.
    if cache_key not in _PREP_KERNEL_CACHE:
        sizeof_element = int(input_a.element_size())
        sA_m, sA_k = map(int, input_a.stride())
        sB_0, sB_n, sB_k = map(int, input_b.stride())
        sC_m, sC_n = map(int, {{get_output()}}.stride())

        prepare_metatadata = cute.compile(
            launch_build_group_ptrs_from_bases,
            base_A_u64=base_A_u64,
            base_B_u64=base_B_u64,
            base_C_u64=base_C_u64,
            offs=from_dlpack(input_a_offs),
            G=int(G),
            K=int(K),
            N=int(N),
            sizeof_element=sizeof_element,
            stride_A_m_elems=sA_m,
            stride_A_k_elems=sA_k,
            stride_B0_elems=sB_0,
            stride_Bk_elems=sB_k,
            stride_Bn_elems=sB_n,
            stride_C_m_elems=sC_m,
            stride_C_n_elems=sC_n,
            BLOCK_M = TILE_M,
            BLOCK_N = TILE_N,
            out_ptrs=ptrs,
            out_problem=probs,
            out_strides_abc=strides,
            stream=stream,
            output_total_num_clusters=cute_num_total_clusters,
        )
        _PREP_KERNEL_CACHE[cache_key] = prepare_metatadata
    else:
        prepare_metatadata = _PREP_KERNEL_CACHE[cache_key]

    prepare_metatadata(
        base_A_u64=base_A_u64,
        base_B_u64=base_B_u64,
        base_C_u64=base_C_u64,
        offs=from_dlpack(input_a_offs),
        out_ptrs=ptrs,
        out_problem=probs,
        out_strides_abc=strides,
        stream=stream,
        output_total_num_clusters=cute_num_total_clusters,
    )

    global _HARDWARE_INFO
    if _HARDWARE_INFO is None:
        hardware_info = cutlass.utils.HardwareInfo()
        _HARDWARE_INFO = hardware_info
    else:
        hardware_info = _HARDWARE_INFO

    if cache_key not in _SCHEDULE_METATA_CACHE:
        sm_count = hardware_info.get_max_active_clusters(1)
        max_active_clusters = hardware_info.get_max_active_clusters(
            cluster_shape_mn[0] * cluster_shape_mn[1]
        )
        _SCHEDULE_METATA_CACHE[cache_key] = (sm_count, max_active_clusters)
    else:
        sm_count, max_active_clusters = _SCHEDULE_METATA_CACHE[cache_key]

    # Prepare tensormap buffer for each SM
    num_tensormap_buffers = sm_count
    tensormap_shape = (
        num_tensormap_buffers,
        GroupedGemmKernel.num_tensormaps,
        GroupedGemmKernel.bytes_per_tensormap // 8,
    )

    torch_tensor_of_tensormap = torch.empty(
        tensormap_shape, dtype=torch.int64, device=device
    )
    cute_tensor_of_tensormap = from_dlpack(torch_tensor_of_tensormap, assumed_align=16)

    # Compute cluster tile shape
    def compute_cluster_tile_shape(
        mma_tiler_mn: tuple[int, int],
        cluster_shape_mn: tuple[int, int],
        use_2cta_instrs: bool,
    ) -> tuple[int, int]:
        cta_tile_shape_mn = list(mma_tiler_mn)
        if use_2cta_instrs:
            cta_tile_shape_mn[0] = cta_tile_shape_mn[0] // 2
        return tuple(x * y for x, y in zip(cta_tile_shape_mn, cluster_shape_mn))

    cluster_tile_shape_mn = compute_cluster_tile_shape(
        mma_tiler_mn,
        cluster_shape_mn,
        use_2cta_instrs,
    )

    # TODO(shikaili): Rewrite a dynamic tile scheduler.
    # Compute total number of cluster tiles we need to compute for given grouped GEMM problem
    def cdiv(x, y) -> int:
        return (x + y - 1) // y

    def compute_estimate_total_num_clusters(
        problem_type: int,
        problem_sizes_gmnk: tuple[int, int, int, int],
        cluster_tile_shape_mn: tuple[int, int],
    ) -> int:
        G, M, N = problem_sizes_gmnk[:3]
        BLOCK_SIZE_M, BLOCK_SIZE_N = cluster_tile_shape_mn
        if problem_type == _FPROP or problem_type == _DGRAD:
            estimate_total_num_clusters = cdiv(M, BLOCK_SIZE_M) * cdiv(N, BLOCK_SIZE_N)
        else:
            estimate_total_num_clusters = (
                G * cdiv(M, BLOCK_SIZE_M) * cdiv(N, BLOCK_SIZE_M)
            )
        return estimate_total_num_clusters

    estimate_total_num_clusters = compute_estimate_total_num_clusters(
        problem_type,
        problem_sizes_gmnk,
        cluster_tile_shape_mn,
    )

    # Initialize Stream
    current_stream = cutlass_torch.current_stream()

    grouped_gemm = GroupedGemmKernel(
        acc_dtype,
        use_2cta_instrs,
        mma_tiler_mn,
        cluster_shape_mn,
        tensormap_update_mode,
    )

    # Compile grouped GEMM kernel
    if cache_key not in _MAIN_KERNEL_CACHE:
        compiled_grouped_gemm = cute.compile(
            grouped_gemm,
            initial_a=cute_tensors_abc[0],
            initial_b=cute_tensors_abc[1],
            initial_c=cute_tensors_abc[2],
            group_count=num_groups,
            problem_shape_mnkl=probs,
            strides_abc=strides,
            tensor_address_abc=ptrs,
            estimate_total_num_clusters=estimate_total_num_clusters,
            total_num_clusters=cute_num_total_clusters,
            tensormap_cute_tensor=cute_tensor_of_tensormap,
            max_active_clusters=max_active_clusters,
            stream=current_stream,
            output_accum=output_accum,
        )
        _MAIN_KERNEL_CACHE[cache_key] = compiled_grouped_gemm
    else:
        compiled_grouped_gemm = _MAIN_KERNEL_CACHE[cache_key]

    compiled_grouped_gemm(
        initial_a=cute_tensors_abc[0],
        initial_b=cute_tensors_abc[1],
        initial_c=cute_tensors_abc[2],
        problem_shape_mnkl=probs,
        strides_abc=strides,
        tensor_address_abc=ptrs,
        estimate_total_num_clusters=estimate_total_num_clusters,
        total_num_clusters=cute_num_total_clusters,
        tensormap_cute_tensor=cute_tensor_of_tensormap,
        stream=current_stream,
    )
