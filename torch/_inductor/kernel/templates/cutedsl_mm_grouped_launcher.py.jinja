import functools
from torch._inductor.runtime.runtime_utils import ceildiv
from cutlass.utils import TensorMapUpdateMode
{{gen_defines()}}
from torch._inductor.kernel.templates.cutedsl_mm_grouped_kernel import GroupedGemmKernel

# Note about caching:
# Each instantiated CuTeDSL grouped GEMM kernel file generated by Inductor
# maintains its own local caching system. At this stage, all compile-time
# constexprs (e.g., TILE_M, TILE_N, CLUSTER_M/N, USE_2_CTA) and the kernel
# name itself ({{kernel_name}}) are permanently baked into the file, so they
# do not need to be included in any cache key.
#
# Caching is organized into three layers:
#
#   1. _PREP_KERNEL_CACHE
#      Caches the compiled executor for launch_build_group_ptrs_from_bases().
#      This kernel:
#        - Computes per-group (m_g, n, k, 1) problem sizes from `offs`,
#        - Materializes A/B/C pointers and strides, and
#        - Computes the exact total_num_clusters in cluster tiles on device.
#      The cache key (`cache_key`) depends on problem_type, dtypes, layout
#      hints (leading_dims), the shape key (G, M/None, N, K/None), the dtype
#      of `input_a_offs`, and output_accum. Different `offs` contents reuse
#      the same compiled kernel as long as shapes and dtypes are unchanged.
#
#   2. _MAIN_KERNEL_CACHE
#      Caches the compiled GroupedGemmKernel executor. It uses the same
#      `cache_key`, so a given combination of problem_type, shape key,
#      dtypes, and layout flags maps to a single compiled persistent grouped
#      GEMM kernel. For each invocation, we pass:
#        - estimate_total_num_clusters: a cheap host estimate, and
#        - total_num_clusters: the device-computed scalar from the prep kernel.
#      This allows one compiled GEMM kernel to be reused across different
#      group partitions (`offs`), while the scheduler still sees the exact
#      cluster count for the current run, without any device-to-host sync.
#
#   3. _HARDWARE_INFO and _SCHEDULE_META_CACHE
#      HardwareInfo.get_max_active_clusters() is relatively expensive and
#      depends only on the GPU. We:
#        - Cache a HardwareInfo instance in _HARDWARE_INFO, and
#        - Cache (sm_count, max_active_clusters) in _SCHEDULE_META_CACHE,
#          keyed by `cache_key`.
#      This avoids recomputing occupancy limits when compiling or reusing
#      executors for new shapes and group partitions.


_FPROP = 0
_DGRAD = 1
_WGRAD = 2

_HARDWARE_INFO = None
_SCHEDULE_META_CACHE = {}
_PREP_KERNEL_CACHE = {}
_MAIN_KERNEL_CACHE = {}

def cdiv(x, y) -> int:
    return (x + y - 1) // y

def compute_cluster_tile_shape(
    mma_tiler_mn,
    cluster_shape_mn,
    use_2cta_instrs,
):
    cta_tile_shape_mn = list(mma_tiler_mn)
    if use_2cta_instrs:
        cta_tile_shape_mn[0] = cta_tile_shape_mn[0] // 2
    return tuple(x * y for x, y in zip(cta_tile_shape_mn, cluster_shape_mn))


@cute.kernel
def build_group_ptrs_from_bases_kernel(
    base_A_u64: cutlass.Int64,  # device addr of input_a (bytes)
    base_B_u64: cutlass.Int64,  # device addr of input_b (bytes)
    base_C_u64: cutlass.Int64,  # device addr of Output (bytes)
    offs: cute.Tensor,  # [G], cutlass.Int32/64 cumulative
    K: cutlass.Constexpr,
    N: cutlass.Constexpr,
    sizeof_element: cutlass.Int32,  # bytes
    # -------- STRIDES (in ELEMENTS) --------
    stride_A_m_elems: cutlass.Constexpr,  # A.stride(0)
    stride_A_k_elems: cutlass.Constexpr,  # A.stride(1)
    stride_B0_elems: cutlass.Constexpr,  # B.stride(0)
    stride_Bk_elems: cutlass.Constexpr,  # B.stride(1)
    stride_Bn_elems: cutlass.Constexpr,  # B.stride(2)
    stride_C_m_elems: cutlass.Constexpr,  # C.stride(0)
    stride_C_n_elems: cutlass.Constexpr,  # C.stride(1)
    # -------- CLUSTER CONFIG --------
    CLUSTER_TILE_M: cutlass.Constexpr,
    CLUSTER_TILE_N: cutlass.Constexpr,
    # -------- OUTPUTS --------
    out_ptrs: cute.Tensor,  # [G,3] cutlass.Int64: (A_ptr, B_ptr, C_ptr)
    out_problem: cute.Tensor,  # [G,4] cutlass.Int32: (m_g, n, k, 1)
    out_strides_abc: cute.Tensor,  # [G,3,2] cutlass.Int32 [[A_m,A_k],[B_n,B_k],[C_m,C_n]]
    output_total_num_clusters: cute.Tensor,
):
    tidx, _, _ = cute.arch.thread_idx()
    g = tidx

    m_beg_i32 = 0
    if g > 0:
        m_beg_i32 = offs[g - 1]
    m_end_i32 = offs[g]
    m_g_i32 = m_end_i32 - m_beg_i32

    a_byte_off = (
        cutlass.Int64(m_beg_i32) * stride_A_m_elems * cutlass.Int64(sizeof_element)
    )
    c_byte_off = (
        cutlass.Int64(m_beg_i32) * stride_C_m_elems * cutlass.Int64(sizeof_element)
    )
    b_byte_off = cutlass.Int64(g) * stride_B0_elems * cutlass.Int64(sizeof_element)

    # ---- pointers ----
    out_ptrs[g, 0] = base_A_u64 + a_byte_off
    out_ptrs[g, 1] = base_B_u64 + b_byte_off
    out_ptrs[g, 2] = base_C_u64 + c_byte_off

    # ---- (m, n, k, 1) ----
    out_problem[g, 0] = m_g_i32
    out_problem[g, 1] = N
    out_problem[g, 2] = K
    out_problem[g, 3] = cutlass.Int32(1)

    # ---- strides ----
    out_strides_abc[g, 0, 0] = cutlass.Int32(stride_A_m_elems)
    out_strides_abc[g, 0, 1] = cutlass.Int32(stride_A_k_elems)
    out_strides_abc[g, 1, 0] = cutlass.Int32(stride_Bn_elems)
    out_strides_abc[g, 1, 1] = cutlass.Int32(stride_Bk_elems)
    out_strides_abc[g, 2, 0] = cutlass.Int32(stride_C_m_elems)
    out_strides_abc[g, 2, 1] = cutlass.Int32(stride_C_n_elems)

    if tidx == 0:
        total_clusters = cutlass.Int32(0)
        num_groups_static = out_problem.shape[0]

        for i in range(num_groups_static):
            i_beg = 0
            if i > 0:
                i_beg = offs[i - 1]
            i_end = offs[i]
            m_size = i_end - i_beg

            m_clusters = (m_size + CLUSTER_TILE_M - 1) // CLUSTER_TILE_M
            n_clusters = (N + CLUSTER_TILE_N - 1) // CLUSTER_TILE_N

            total_clusters += m_clusters * n_clusters

        output_total_num_clusters[0] = total_clusters


@cute.jit
def launch_build_group_ptrs_from_bases(
    base_A_u64: cutlass.Int64,
    base_B_u64: cutlass.Int64,
    base_C_u64: cutlass.Int64,
    offs: cute.Tensor,
    G: cutlass.Constexpr,
    K: cutlass.Constexpr,
    N: cutlass.Constexpr,
    sizeof_element: cutlass.Constexpr,
    stride_A_m_elems: cutlass.Constexpr,
    stride_A_k_elems: cutlass.Constexpr,
    stride_B0_elems: cutlass.Constexpr,
    stride_Bk_elems: cutlass.Constexpr,
    stride_Bn_elems: cutlass.Constexpr,
    stride_C_m_elems: cutlass.Constexpr,
    stride_C_n_elems: cutlass.Constexpr,
    CLUSTER_TILE_M: cutlass.Constexpr,
    CLUSTER_TILE_N: cutlass.Constexpr,
    out_ptrs: cute.Tensor,  # [G,3] cutlass.Int64
    out_problem: cute.Tensor,  # [G,4] cutlass.Int32
    out_strides_abc: cute.Tensor,  # [3,2] cutlass.Int32
    output_total_num_clusters: cute.Tensor,
    stream: cuda.CUstream,
):
    build_group_ptrs_from_bases_kernel(
        base_A_u64,
        base_B_u64,
        base_C_u64,
        offs,
        K,
        N,
        sizeof_element,
        stride_A_m_elems,
        stride_A_k_elems,
        stride_B0_elems,
        stride_Bk_elems,
        stride_Bn_elems,
        stride_C_m_elems,
        stride_C_n_elems,
        CLUSTER_TILE_M,
        CLUSTER_TILE_N,
        out_ptrs,
        out_problem,
        out_strides_abc,
        output_total_num_clusters,
    ).launch(grid=(1, 1, 1), block=(G, 1, 1), stream=stream)

{{def_kernel("input_a", "input_b", "input_a_offs")}}
    stream = cuda.CUstream(stream)

     # Treat this Inductor path as FPROP; DGRAD/WGRAD routing is future work.
    problem_type = _FPROP

    input_b = input_b.transpose(1, 2)

    G, N, K = input_b.shape
    M = input_a.shape[0]

    problem_sizes_gmnk = (G, M, N, K)
    num_groups = G

    torch_tensors_abc = (input_a, input_b, {{get_output()}})

    mma_tiler_mn = (TILE_M, TILE_N)
    cluster_shape_mn = (CLUSTER_M, CLUSTER_N)
    use_2cta_instrs = USE_2_CTA
    output_accum = False # Assume Inductor uses non-accumulating writes for this grouped GEMM.

    if problem_type == _FPROP:
        shape_key = (G, None, N, K)
        leading_dims = (1, 1, 1)
    elif problem_type == _DGRAD:
        shape_key = (G, None, N, K)
        leading_dims = (1, 0, 1)
    elif problem_type == _WGRAD:
        shape_key = (G, M, N, None)
        leading_dims = (0, 0, 1)
    else:
        raise ValueError("Unrecognized problem type: ", problem_type)

    cute_tensors_abc = [
        from_dlpack(input_a.unsqueeze(-1), assumed_align=16).mark_layout_dynamic(leading_dim=leading_dims[0]),
        # TODO(nikhilap): Inductor passes B transposed; with current mark_layout_dynamic limits we can't safely mark any dims dynamic yet.
        from_dlpack(input_b[0].unsqueeze(-1), assumed_align=16),
        from_dlpack({{get_output()}}.unsqueeze(-1), assumed_align=16).mark_layout_dynamic(leading_dim=leading_dims[2]),
    ]
    offs_cute = from_dlpack(input_a_offs, assumed_align=16)

    device = torch_tensors_abc[0].device

    ptrs_t = torch.empty((G, 3), device=device, dtype=torch.int64)
    probs_t = torch.empty((G, 4), device=device, dtype=torch.int32)
    strides_t = torch.empty((G, 3, 2), device=device, dtype=torch.int32)
    num_total_clusters_t = torch.empty((1,), dtype=torch.int32, device=device)
    ptrs = from_dlpack(ptrs_t)
    probs = from_dlpack(probs_t)
    strides = from_dlpack(strides_t)
    num_total_clusters = from_dlpack(num_total_clusters_t, assumed_align=16)

    cache_key = (
        problem_type,
        tuple(t.dtype for t in torch_tensors_abc),
        input_a_offs.dtype,
        shape_key,
        leading_dims,
        output_accum,
    )

    base_A_u64 = int(input_a.data_ptr())
    base_B_u64 = int(input_b.data_ptr())
    base_C_u64 = int({{get_output()}}.data_ptr())

    cluster_tile_shape_mn = compute_cluster_tile_shape(
        mma_tiler_mn,
        cluster_shape_mn,
        use_2cta_instrs,
    )

    # TODO(shikaili): Move prepare metadata into main JIT function.
    if cache_key not in _PREP_KERNEL_CACHE:
        sizeof_element = int(input_a.element_size())
        sA_m, sA_k = map(int, input_a.stride())
        sB_0, sB_n, sB_k = map(int, input_b.stride())
        sC_m, sC_n = map(int, {{get_output()}}.stride())

        CLUSTER_TILE_M, CLUSTER_TILE_N = map(int, cluster_tile_shape_mn)

        prepare_metadata = cute.compile(
            launch_build_group_ptrs_from_bases,
            base_A_u64=base_A_u64,
            base_B_u64=base_B_u64,
            base_C_u64=base_C_u64,
            offs=offs_cute,
            G=int(G),
            K=int(K),
            N=int(N),
            sizeof_element=sizeof_element,
            stride_A_m_elems=sA_m,
            stride_A_k_elems=sA_k,
            stride_B0_elems=sB_0,
            stride_Bk_elems=sB_k,
            stride_Bn_elems=sB_n,
            stride_C_m_elems=sC_m,
            stride_C_n_elems=sC_n,
            CLUSTER_TILE_M=CLUSTER_TILE_M,
            CLUSTER_TILE_N=CLUSTER_TILE_N,
            out_ptrs=ptrs,
            out_problem=probs,
            out_strides_abc=strides,
            output_total_num_clusters=num_total_clusters,
            stream=stream,
        )

        _PREP_KERNEL_CACHE[cache_key] = prepare_metadata
    else:
        prepare_metadata = _PREP_KERNEL_CACHE[cache_key]

    prepare_metadata(
        base_A_u64=base_A_u64,
        base_B_u64=base_B_u64,
        base_C_u64=base_C_u64,
        offs=offs_cute,
        out_ptrs=ptrs,
        out_problem=probs,
        out_strides_abc=strides,
        output_total_num_clusters=num_total_clusters,
        stream=stream,
    )

    global _HARDWARE_INFO
    if _HARDWARE_INFO is None:
        hardware_info = cutlass.utils.HardwareInfo()
        _HARDWARE_INFO = hardware_info
    else:
        hardware_info = _HARDWARE_INFO

    if cache_key not in _SCHEDULE_META_CACHE:
        sm_count = hardware_info.get_max_active_clusters(1)
        max_active_clusters = hardware_info.get_max_active_clusters(
            cluster_shape_mn[0] * cluster_shape_mn[1]
        )
        _SCHEDULE_META_CACHE[cache_key] = (sm_count, max_active_clusters)
    else:
        sm_count, max_active_clusters = _SCHEDULE_META_CACHE[cache_key]

    # Prepare tensormap buffer for each SM
    num_tensormap_buffers = sm_count
    tensormap_shape = (
        num_tensormap_buffers,
        GroupedGemmKernel.num_tensormaps,
        GroupedGemmKernel.bytes_per_tensormap // 8,
    )

    torch_tensor_of_tensormap = torch.empty(
        tensormap_shape, dtype=torch.int64, device=device
    )
    cute_tensor_of_tensormap = from_dlpack(torch_tensor_of_tensormap, assumed_align=16)

    # TODO(shikaili): Rewrite a dynamic tile scheduler.
    # Compute total number of cluster tiles we need to compute for given grouped GEMM problem
    def compute_estimate_total_num_clusters(
        problem_type: int,
        problem_sizes_gmnk: tuple[int, int, int, int],
        cluster_tile_shape_mn: tuple[int, int],
    ) -> int:
        G, M, N = problem_sizes_gmnk[:3]
        BLOCK_SIZE_M, BLOCK_SIZE_N = cluster_tile_shape_mn
        if problem_type == _FPROP or problem_type == _DGRAD:
            estimate_total_num_clusters = cdiv(M, BLOCK_SIZE_M) * cdiv(N, BLOCK_SIZE_N)
        else:
            estimate_total_num_clusters = (
                G * cdiv(M, BLOCK_SIZE_M) * cdiv(N, BLOCK_SIZE_M)
            )
        return estimate_total_num_clusters

    estimate_total_num_clusters = compute_estimate_total_num_clusters(
        problem_type,
        problem_sizes_gmnk,
        cluster_tile_shape_mn,
    )

    grouped_gemm = GroupedGemmKernel(
        ACC_DTYPE,
        use_2cta_instrs,
        mma_tiler_mn,
        cluster_shape_mn,
        TENSORMAP_UPDATE_MODE,
    )

    # Compile grouped GEMM kernel
    if cache_key not in _MAIN_KERNEL_CACHE:
        compiled_grouped_gemm = cute.compile(
            grouped_gemm,
            initial_a=cute_tensors_abc[0],
            initial_b=cute_tensors_abc[1],
            initial_c=cute_tensors_abc[2],
            group_count=num_groups,
            problem_shape_mnkl=probs,
            strides_abc=strides,
            tensor_address_abc=ptrs,
            estimate_total_num_clusters=estimate_total_num_clusters,
            total_num_clusters=num_total_clusters,
            tensormap_cute_tensor=cute_tensor_of_tensormap,
            max_active_clusters=max_active_clusters,
            stream=stream,
            output_accum=output_accum,
        )
        _MAIN_KERNEL_CACHE[cache_key] = compiled_grouped_gemm
    else:
        compiled_grouped_gemm = _MAIN_KERNEL_CACHE[cache_key]

    compiled_grouped_gemm(
        initial_a=cute_tensors_abc[0],
        initial_b=cute_tensors_abc[1],
        initial_c=cute_tensors_abc[2],
        problem_shape_mnkl=probs,
        strides_abc=strides,
        tensor_address_abc=ptrs,
        estimate_total_num_clusters=estimate_total_num_clusters,
        total_num_clusters=num_total_clusters,
        tensormap_cute_tensor=cute_tensor_of_tensormap,
        stream=stream,
    )
